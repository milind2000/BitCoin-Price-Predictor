<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>Info</title>
    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">

    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>

  </head>
  <body>
   <div class = "container">

      <h1 style="text-align: left;">&nbsp;Illustrated Guide to LSTM’s : A step by step explanation</h1><div><br /></div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-FKMY3pSI63U/X372iUFLCAI/AAAAAAAAGz4/7PAAPwIigSIYs1F8LqbPANnfW-qgEs33gCLcBGAsYHQ/s875/1_n-IgHZM5baBUjq0T7RYDBw.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="492" data-original-width="875" src="https://1.bp.blogspot.com/-FKMY3pSI63U/X372iUFLCAI/AAAAAAAAGz4/7PAAPwIigSIYs1F8LqbPANnfW-qgEs33gCLcBGAsYHQ/s320/1_n-IgHZM5baBUjq0T7RYDBw.gif" width="320" /></a></div><div><h1 class="if ig dp cf ih ii ij ik il im in io ip iq ir is it iu iv iw ix eh" data-selectable-paragraph="" id="2875" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 36px; letter-spacing: -0.022em; line-height: 40px; margin: 1.95em 0px -0.28em;">The Problem, Short-term Memory</h1><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="f5ec" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions, RNN’s may leave out important information from the beginning.</p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="0155" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">During back propagation, recurrent neural networks suffer from the vanishing gradient problem. Gradients are values used to update a neural networks weights. The vanishing gradient problem is when the gradient shrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn’t contribute too much learning.</p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="0155" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span style="letter-spacing: -0.003em;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><a href="https://1.bp.blogspot.com/-fVxNCxtDryI/X3727_NMsSI/AAAAAAAAG0A/jHLBEZycgG4fuOPfChuht57mXOIuYGCSwCLcBGAsYHQ/s875/1_PYiQa_bNzM8ugYz_D1yvgw.png" style="background-color: transparent; letter-spacing: -0.003em; margin-left: 1em; margin-right: 1em; text-align: center;"><img border="0" data-original-height="244" data-original-width="875" src="https://1.bp.blogspot.com/-fVxNCxtDryI/X3727_NMsSI/AAAAAAAAG0A/jHLBEZycgG4fuOPfChuht57mXOIuYGCSwCLcBGAsYHQ/s320/1_PYiQa_bNzM8ugYz_D1yvgw.png" width="320" /></a></p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="0155" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;"><br /></p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="0155" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.063px;">So in recurrent neural networks, layers that get a small gradient update stops learning. Those are usually the earlier layers. So because these layers don’t learn, RNN’s can forget what it seen in longer sequences, thus having a short-term memory. If you want to know more about the mechanics of recurrent neural networks in general, you can read my previous post here.</span></p><h1 class="if ig dp cf ih ii ij ik il im in io ip iq ir is it iu iv iw ix eh" data-selectable-paragraph="" id="25b5" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 36px; letter-spacing: -0.022em; line-height: 40px; margin: 1.95em 0px -0.28em;">LSTM’s<span class="ay" style="box-sizing: inherit; font-weight: inherit;">&nbsp;as a solution</span></h1><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="74b2" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">LSTM ’s and GRU’s were created as the solution to short-term memory. They have internal mechanisms called gates that can regulate the flow of information.</p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="74b2" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">&nbsp; &nbsp; &nbsp; &nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-XnEDI30xFA8/X373iPmS_5I/AAAAAAAAG0I/N96DvRAmXXooAfPWKQnZPr7nqChW0jhKQCLcBGAsYHQ/s875/1_yBXV9o5q7L_CvY7quJt3WQ.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="556" data-original-width="875" height="228" src="https://1.bp.blogspot.com/-XnEDI30xFA8/X373iPmS_5I/AAAAAAAAG0I/N96DvRAmXXooAfPWKQnZPr7nqChW0jhKQCLcBGAsYHQ/w465-h228/1_yBXV9o5q7L_CvY7quJt3WQ.png" width="465" /></a></div><br /><p></p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="873e" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">These gates can learn which data in a sequence is important to keep or throw away. By doing that, it can pass relevant information down the long chain of sequences to make predictions. Almost all state of the art results based on recurrent neural networks are achieved with these two networks. LSTM’s and GRU’s can be found in speech recognition, speech synthesis, and text generation. You can even use them to generate captions for videos.</p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="f810" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">Ok, so by the end of this post you should have a solid understanding of why LSTM’s and GRU’s are good at processing long sequences. I am going to approach this with intuitive explanations and illustrations and avoid as much math as possible.</p><h1 class="if ig dp cf ih ii ij ik il im in io ip iq ir is it iu iv iw ix eh" data-selectable-paragraph="" id="fa88" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 36px; letter-spacing: -0.022em; line-height: 40px; margin: 1.95em 0px -0.28em;">Intuition</h1><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="206b" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">Ok, Let’s start with a thought experiment. Let’s say you’re looking at reviews online to determine if you want to buy Life cereal (don’t ask me why). You’ll first read the review then determine if someone thought it was good or if it was bad.</p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="206b" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.003em;">When you read the review, your brain subconsciously only remembers important keywords. You pick up words like “amazing” and “perfectly balanced breakfast”. You don’t care much for words like “this”, “gave“, “all”, “should”, etc. If a friend asks you the next day what the review said, you probably wouldn’t remember it word for word. You might remember the main points though like “will definitely be buying again”. If you’re a lot like me, the other words will fade away from memory.</span></p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="9158" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">And that is essentially what an LSTM or GRU does. It can learn to keep only relevant information to make predictions, and forget non relevant data. In this case, the words you remembered made you judge that it was good.</p><h1 class="if ig dp cf ih ii ij ik il im in io ip iq ir is it iu iv iw ix eh" data-selectable-paragraph="" id="03ee" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 36px; letter-spacing: -0.022em; line-height: 40px; margin: 1.95em 0px -0.28em;">Review of Recurrent Neural Networks</h1><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">To understand how LSTM’s or GRU’s achieves this, let’s review the recurrent neural network. An RNN works like this; First words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one.</p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><br /></p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-SJleZckeeLU/X374FdH_A-I/AAAAAAAAG0Q/iHGP-CPJP2sWc0L64G0z2BTRX4c3HeyxQCLcBGAsYHQ/s875/1_AQ52bwW55GsJt6HTxPDuMA.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="230" data-original-width="875" height="138" src="https://1.bp.blogspot.com/-SJleZckeeLU/X374FdH_A-I/AAAAAAAAG0Q/iHGP-CPJP2sWc0L64G0z2BTRX4c3HeyxQCLcBGAsYHQ/w401-h138/1_AQ52bwW55GsJt6HTxPDuMA.gif" width="401" /></a></div><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><br /></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.063px;">While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.</span></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.063px;"><br /></span></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"></p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-k0ZPzcAwP_w/X374diI-KbI/AAAAAAAAG0Y/UNcP02Dtj3I9Q9tD-VIf6_y1s7mNnVXtwCLcBGAsYHQ/s875/1_o-Cq5U8-tfa1_ve2Pf3nfg.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="230" data-original-width="875" height="110" src="https://1.bp.blogspot.com/-k0ZPzcAwP_w/X374diI-KbI/AAAAAAAAG0Y/UNcP02Dtj3I9Q9tD-VIf6_y1s7mNnVXtwCLcBGAsYHQ/w320-h110/1_o-Cq5U8-tfa1_ve2Pf3nfg.gif" width="320" /></a></div><br /><p></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.063px;">Let’s look at a cell of the RNN to see how you would calculate the hidden state. First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network.</span></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="0557" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"></p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-vhe_UgxiBNQ/X3741zalp1I/AAAAAAAAG0k/EOT_KTs_5rU5ZZscDetcdopi3IV6zZ6oQCLcBGAsYHQ/s950/1_WMnFSJHzOloFlJHU6fVN-g.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="376" data-original-width="950" height="169" src="https://1.bp.blogspot.com/-vhe_UgxiBNQ/X3741zalp1I/AAAAAAAAG0k/EOT_KTs_5rU5ZZscDetcdopi3IV6zZ6oQCLcBGAsYHQ/w291-h169/1_WMnFSJHzOloFlJHU6fVN-g.gif" width="291" /></a></div><p></p><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="206d" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Tanh activation</h2><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="acfa" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">The tanh activation is used to help regulate the values flowing through the network. The tanh function squishes values to always be between -1 and 1.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-2Ctwc9D43GE/X375JIAtkYI/AAAAAAAAG0s/E9bI45sQXxowtsZ6E7AfGnG8tvIOjugjwCLcBGAsYHQ/s950/1_iRlEg1GBKRzGTre5aOQUCg.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="376" data-original-width="950" height="184" src="https://1.bp.blogspot.com/-2Ctwc9D43GE/X375JIAtkYI/AAAAAAAAG0s/E9bI45sQXxowtsZ6E7AfGnG8tvIOjugjwCLcBGAsYHQ/w367-h184/1_iRlEg1GBKRzGTre5aOQUCg.gif" width="367" /></a></div><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="acfa" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.063px;">When vectors are flowing through a neural network, it undergoes many transformations due to various math operations. So imagine a value that continues to be multiplied by let’s say&nbsp;</span><span class="hi lb" style="box-sizing: inherit; font-weight: 700; letter-spacing: -0.063px;"><em class="lc" style="box-sizing: inherit;">3</em></span><span style="letter-spacing: -0.063px;">. You can see how some values can explode and become astronomical, causing other values to seem insignificant.</span></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="acfa" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"></p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-Dop_XQ7YDxA/X375a4t5rBI/AAAAAAAAG00/Dhfz9U7xSwglFNl27TvqJh-YDuKPsNiOgCLcBGAsYHQ/s950/1_LgbEFcGiUpseZ--M7wuZhg.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="126" data-original-width="950" height="96" src="https://1.bp.blogspot.com/-Dop_XQ7YDxA/X375a4t5rBI/AAAAAAAAG00/Dhfz9U7xSwglFNl27TvqJh-YDuKPsNiOgCLcBGAsYHQ/w320-h96/1_LgbEFcGiUpseZ--M7wuZhg.gif" width="320" /></a></div><span style="letter-spacing: -0.063px;"><br /></span><p></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="acfa" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.063px;">A tanh function ensures that the values stay between -1 and 1, thus regulating the output of the neural network. You can see how the same values from above remain between the boundaries allowed by the tanh function.</span></p><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="acfa" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"></p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-vY7ecwoMa7w/X375lTgwxqI/AAAAAAAAG04/XTmcgUr5Se0TdMrPKofll7B4_iH8r3r1gCLcBGAsYHQ/s950/1_gFC2bTg3uihp1klknWU0qg.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="126" data-original-width="950" height="90" src="https://1.bp.blogspot.com/-vY7ecwoMa7w/X375lTgwxqI/AAAAAAAAG04/XTmcgUr5Se0TdMrPKofll7B4_iH8r3r1gCLcBGAsYHQ/w320-h90/1_gFC2bTg3uihp1klknWU0qg.gif" width="320" /></a></div><p></p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="1cba" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">So that’s an RNN. It has very few operations internally but works pretty well given the right circumstances (like short sequences). RNN’s uses a lot less computational resources than it’s evolved variants, LSTM’s and GRU’s.</p><h1 class="if ig dp cf ih ii ij ik il im in io ip iq ir is it iu iv iw ix eh" data-selectable-paragraph="" id="703f" style="background-color: white; box-sizing: inherit; color: #292929; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 36px; letter-spacing: -0.022em; line-height: 40px; margin: 1.95em 0px -0.28em;">LSTM</h1></div><div><br /></div><div><span face="medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif" style="background-color: white; color: #292929; font-size: 21px; letter-spacing: -0.063px;">An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM’s cells.</span></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-PKb4NWXxD1Q/X375zYn_tVI/AAAAAAAAG1A/7S7dbQaNWS8xbV0egNmqGEwKmZBlB8c-ACLcBGAsYHQ/s875/1_0f8r3Vd-i4ueYND1CUrhMA.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="523" data-original-width="875" src="https://1.bp.blogspot.com/-PKb4NWXxD1Q/X375zYn_tVI/AAAAAAAAG1A/7S7dbQaNWS8xbV0egNmqGEwKmZBlB8c-ACLcBGAsYHQ/s320/1_0f8r3Vd-i4ueYND1CUrhMA.png" width="320" /></a></div><br /><div><br /></div><div><span face="medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif" style="background-color: white; color: #292929; font-size: 21px; letter-spacing: -0.063px;">These operations are used to allow the LSTM to keep or forget information. Now looking at these operations can get a little overwhelming so we’ll go over this step by step.</span></div><div><span face="medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif" style="background-color: white; color: #292929; font-size: 21px; letter-spacing: -0.063px;"><br /></span></div><div><span face="medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif" style="background-color: white; color: #292929; letter-spacing: -0.063px;"><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="7838" style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Core Concept</h2><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="7029" style="box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">The core concept of LSTM’s are the cell state, and it’s various gates. The cell state act as a transport highway that transfers relative information all the way down the sequence chain. You can think of it as the “memory” of the network. The cell state, in theory, can carry relevant information throughout the processing of the sequence. So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get’s added or removed to the cell state via gates. The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training.</p><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="0ba2" style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Sigmoid</h2><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="52c8" style="box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">Gates contains sigmoid activations. A sigmoid activation is similar to the tanh activation. Instead of squishing values between -1 and 1, it squishes values between 0 and 1. That is helpful to update or forget data because any number getting multiplied by 0 is 0, causing values to disappears or be “forgotten.” Any number multiplied by 1 is the same value therefore that value stay’s the same or is “kept.” The network can learn which data is not important therefore can be forgotten or which data is important to keep.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-FVGCrywMOxQ/X3_qC6PfT2I/AAAAAAAAG1Q/Ms11DcxTXAofhBsLjVSOW5XaP_YLqP79QCLcBGAsYHQ/s950/1_rOFozAke2DX5BmsX2ubovw.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="376" data-original-width="950" height="203" src="https://1.bp.blogspot.com/-FVGCrywMOxQ/X3_qC6PfT2I/AAAAAAAAG1Q/Ms11DcxTXAofhBsLjVSOW5XaP_YLqP79QCLcBGAsYHQ/w376-h203/1_rOFozAke2DX5BmsX2ubovw.gif" width="376" /></a></div><br /><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="52c8" style="box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="letter-spacing: -0.003em;">Let’s dig a little deeper into what the various gates are doing, shall we? So we have three different gates that regulate information flow in an LSTM cell. A forget gate, input gate, and output gate.</span></p><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="64ed" style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Forget gate</h2><div style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em; text-align: left;"><span style="font-size: medium; font-weight: normal;">First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.</span></div><div style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em; text-align: left;"><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-yDq9aHwOq2c/X3_qT6T3c5I/AAAAAAAAG1Y/a1rEUQxhu9k0sGOangCc8mS2nr1onosOACLcBGAsYHQ/s950/1_GjehOa513_BgpDDP6Vkw2Q.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="500" data-original-width="950" src="https://1.bp.blogspot.com/-yDq9aHwOq2c/X3_qT6T3c5I/AAAAAAAAG1Y/a1rEUQxhu9k0sGOangCc8mS2nr1onosOACLcBGAsYHQ/s320/1_GjehOa513_BgpDDP6Vkw2Q.gif" width="320" /></a></div><br /><span style="font-size: medium; font-weight: normal;"><br /></span></div><div style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em; text-align: left;"><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="075e" style="box-sizing: inherit; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Input Gate</h2><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="769b" style="box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.</p></div></span></div><div><span face="medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif" style="background-color: white; color: #292929; font-size: 21px; letter-spacing: -0.063px;"><br /></span></div><div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-uKScFDlM6fA/X3_qpeWpHwI/AAAAAAAAG1g/bYnAjFeVmgs-QXqtqB4u3TZctlqBEHn9gCLcBGAsYHQ/s950/1_TTmYy7Sy8uUXxUXfzmoKbA.gif" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="450" data-original-width="950" src="https://1.bp.blogspot.com/-uKScFDlM6fA/X3_qpeWpHwI/AAAAAAAAG1g/bYnAjFeVmgs-QXqtqB4u3TZctlqBEHn9gCLcBGAsYHQ/s320/1_TTmYy7Sy8uUXxUXfzmoKbA.gif" width="320" /></a></div><br /><span face="medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif" style="background-color: white; color: #292929; letter-spacing: -0.063px;"><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="8a69" style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Cell State</h2><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="3ed2" style="box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state.</p><div class="separator" style="clear: both; font-size: 21px; text-align: center;"><a href="https://1.bp.blogspot.com/-7nTVEZMHHQk/X4ABnjxx--I/AAAAAAAAG1s/CwozOVxtqv8Zp_pwTvbLcbIz7QqF-0aeQCLcBGAsYHQ/s950/1_S0rXIeO_VoUVOyrYHckUWg.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="500" data-original-width="950" src="https://1.bp.blogspot.com/-7nTVEZMHHQk/X4ABnjxx--I/AAAAAAAAG1s/CwozOVxtqv8Zp_pwTvbLcbIz7QqF-0aeQCLcBGAsYHQ/s320/1_S0rXIeO_VoUVOyrYHckUWg.gif" width="320" /></a></div><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="346e" style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Output Gate</h2><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="e1c5" style="box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;">Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.</p><div class="separator" style="clear: both; font-size: 21px; text-align: center;"><a href="https://1.bp.blogspot.com/-uXsb17KiHVU/X4AB1byyvdI/AAAAAAAAG1w/23qnFbRMk8g-FCZpNMBccWFaWeOGzBDQwCLcBGAsYHQ/s950/1_VOXRGhOShoWWks6ouoDN3Q.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="450" data-original-width="950" src="https://1.bp.blogspot.com/-uXsb17KiHVU/X4AB1byyvdI/AAAAAAAAG1w/23qnFbRMk8g-FCZpNMBccWFaWeOGzBDQwCLcBGAsYHQ/s320/1_VOXRGhOShoWWks6ouoDN3Q.gif" width="320" /></a></div><br /><p class="hg hh dp hi b hj iy hl hm hn iz hp hq hr ja ht hu hv jb hx hy hz jc ib ic id dh eh" data-selectable-paragraph="" id="e1c5" style="box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.003em; line-height: 32px; margin: 0.86em 0px -0.46em; word-break: break-word;"><span style="background-color: #e9f2fd; color: currentcolor; letter-spacing: -0.003em;">To review, the Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be.</span></p><h2 class="kp ig dp cf ih kq kr hl ks kt hp ku kv ht kw kx hx ky kz ib la eh" data-selectable-paragraph="" id="aaa1" style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; font-size: 26px; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em;">Code Demo</h2><div style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em; text-align: left;"><span style="font-size: medium;">For those of you who understand better through seeing the code, here is an example using python pseudo code.</span></div><div style="box-sizing: inherit; font-family: medium-content-sans-serif-font, &quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, &quot;Lucida Sans&quot;, Geneva, Arial, sans-serif; letter-spacing: -0.022em; line-height: 32px; margin: 1.72em 0px -0.31em; text-align: left;"><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-hkWYUD22pFQ/X4AClwNjmHI/AAAAAAAAG18/T6sbm4GQVhw0X5o5FeGXadrAy65fgBukQCLcBGAsYHQ/s828/1_p2yXhtxmYflEUrTC1rCoUA.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="744" data-original-width="828" src="https://1.bp.blogspot.com/-hkWYUD22pFQ/X4AClwNjmHI/AAAAAAAAG18/T6sbm4GQVhw0X5o5FeGXadrAy65fgBukQCLcBGAsYHQ/s320/1_p2yXhtxmYflEUrTC1rCoUA.png" width="320" /></a></div><br /><span style="font-size: medium;"><br /></span></div><div style="font-size: 21px;"><span style="color: currentcolor; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; letter-spacing: -0.003em;">1. First, the previous hidden state and the current input get concatenated. We’ll call it&nbsp;</span><em class="lc" style="box-sizing: inherit; color: currentcolor; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; letter-spacing: -0.003em;">combine</em><span style="color: currentcolor; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; letter-spacing: -0.003em;">.</span></div><div style="font-size: 21px;"><mark class="xc xd nd" style="background-color: #e9f2fd; box-sizing: inherit; color: currentcolor; cursor: pointer;"><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="eb1d" style="background-color: white; box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">2.&nbsp;<em class="lc" style="box-sizing: inherit;">Combine</em>&nbsp;get’s fed into the forget layer. This layer removes non-relevant data.<br style="box-sizing: inherit;" />4. A candidate layer is created using&nbsp;<em class="lc" style="box-sizing: inherit;">combine</em>. The candidate holds possible values to add to the cell state.<br style="box-sizing: inherit;" />3.&nbsp;<em class="lc" style="box-sizing: inherit;">Combine</em>&nbsp;also get’s fed into the input layer. This layer decides what data from the candidate should be added to the new cell state.<br style="box-sizing: inherit;" />5. After computing the forget layer, candidate layer, and the input layer, the cell state is calculated using those vectors and the previous cell state.<br style="box-sizing: inherit;" />6. The output is then computed.<br style="box-sizing: inherit;" />7. Pointwise multiplying the output and the new cell state gives us the new hidden state.</p><p class="hg hh dp hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id dh eh" data-selectable-paragraph="" id="cac9" style="background-color: white; box-sizing: inherit; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; letter-spacing: -0.003em; line-height: 32px; margin: 2em 0px -0.46em; word-break: break-word;">That’s it! The control flow of an LSTM network are a few tensor operations and a for loop. You can use the hidden states for predictions. Combining all those mechanisms, an LSTM can choose which information is relevant to remember or forget during sequence processing.</p></mark></div></span></div>

      </div>

  </body>
</html>
